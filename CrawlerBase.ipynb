{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPf+/lraNvq4L6cwbhx/tnS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuananh1006/Web-Scraping-Book/blob/main/CrawlerBase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IDSHwcLV9OTm"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Content:\n",
        "  def __init__(self,url,title,body):\n",
        "    self.url=url\n",
        "    self.title=title\n",
        "    self.body=body\n",
        "  def print(self):\n",
        "    print(\"title:\"+self.title)\n",
        "    print(\"url:\"+self.url)\n",
        "    print(\"Body:\"+self.body)\n",
        "\n"
      ],
      "metadata": {
        "id": "y_anBPRt9Zgb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapeCNN(url):\n",
        "    bs = BeautifulSoup(urlopen(url))\n",
        "    title = bs.find('h1').text\n",
        "    body = bs.find('div', {'class': 'article__content'}).text\n",
        "    print('body: ')\n",
        "    print(body)\n",
        "    return Content(url, title, body)"
      ],
      "metadata": {
        "id": "_NtuoZBl9xAp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapeBrookings(url):\n",
        "    bs = BeautifulSoup(urlopen(url))\n",
        "    title = bs.find('title').text\n",
        "    content=[]\n",
        "    for description in bs.find_all('p', {'class': ['description','Normal']}):\n",
        "      content.append(description.get_text().replace('\\n',''))\n",
        "    body=\"\\n\".join(content)\n",
        "    return Content(url, title, body)\n"
      ],
      "metadata": {
        "id": "cLl6SxHs9zXr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://vnexpress.net/israel-yeu-cau-dan-thuong-so-tan-khoi-25-khu-vuc-o-lebanon-4799966.html'\n"
      ],
      "metadata": {
        "id": "_dMAmQs_91J8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = scrapeBrookings(url)\n",
        "content.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOAozNOF92QC",
        "outputId": "342b2260-8554-4f5e-8696-7efc7fd641c7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title:Israel yêu cầu dân thường sơ tán khỏi 25 khu vực ở Lebanon - Báo VnExpress\n",
            "url:https://vnexpress.net/israel-yeu-cau-dan-thuong-so-tan-khoi-25-khu-vuc-o-lebanon-4799966.html\n",
            "Body:Quân đội Israel phát cảnh báo đến người dân ở 25 địa phương tại miền nam Lebanon, kêu gọi họ lập tức sơ tán để đảm bảo tính mạng.\n",
            "\"Hành động của Hezbollah khiến Lực lượng Phòng vệ Israel (IDF) phải đối phó. Chúng tôi không có ý định làm hại quý vị. Vì an toàn của bản thân, quý vị phải lập tức sơ tán\", đại tá Avichay Adraee, phát ngôn viên tiếng Arab của IDF, thông báo hôm nay.\n",
            "IDF cũng công bố danh sách 25 thành phố, thị trấn và làng mạc ở miền nam Lebanon mà dân thường cần di tản, trong đó có Nabatieh, đô thị lớn hàng đầu khu vực.\n",
            "\"Bất cứ ai ở gần thành viên, cơ sở hạ tầng và vũ khí của Hezbollah đều tự đặt mình vào vòng nguy hiểm\", ông Adraee cho biết thêm, kêu gọi người dân Lebanon di chuyển lên phía bắc và cho biết sẽ cập nhật thông tin về thời điểm họ được trở về nhà sau.\n",
            "IDF ngày 2/10 cũng đưa ra cảnh báo tương tự với 24 làng ở miền nam Lebanon. Một ngày trước đó, quân đội Israel kêu gọi người dân tại 28 khu vực nhanh chóng di tản.\n",
            "Động thái diễn ra giữa lúc xung đột giữa Israel với nhóm vũ trang Hezbollah ngày càng tăng nhiệt. Israel cuối tháng 9 tập kích hạ sát thủ lĩnh tối cao Hassan Nasrallah cùng hàng loạt thành viên cấp cao của nhóm, sau đó mở chiến dịch hạn chế trên bộ vào miền nam Lebanon.\n",
            "IDF đặt mục tiêu loại bỏ các cứ điểm của Hezbollah dọc biên giới Israel - Lebanon, tạo điều kiện để tiến tới thỏa thuận ngoại giao, trong đó nhóm vũ trang sẽ lùi bờ bắc sông Litani như quy định trong nghị quyết của Hội đồng Bảo an Liên Hợp Quốc.\n",
            "Quân đội Israel hôm nay thông báo tập kích hạ 15 tay súng Hezbollah ở khu Bint Jbeil, miền nam Lebanon. Đây là nơi chịu thiệt hại nặng nề trong cuộc chiến giữa hai bên năm 2006.\n",
            "Trong khi đó, Hezbollah tuyên bố phá hủy ba xe tăng Israel đang tiến đến ngôi làng Maroun al-Ras ở biên giới Lebanon, xua đuổi một trực thăng và gây thiệt hại cho nhiều đơn vị đối phương.\n",
            "Như Tâm (Theo AFP, Times of Israel)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    \"\"\"\n",
        "    Contains information about website structure\n",
        "    \"\"\"\n",
        "    def __init__(self, name, url, titleTag, bodyTag):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag"
      ],
      "metadata": {
        "id": "uKhN9ez6APHC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crawler:\n",
        "    def getPage(url):\n",
        "        try:\n",
        "            html = urlopen(url)\n",
        "        except Exception:\n",
        "            return None\n",
        "        return BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    def safeGet(bs, selector):\n",
        "        \"\"\"\n",
        "        Utility function used to get a content string from a Beautiful Soup\n",
        "        object and a selector. Returns an empty string if no object\n",
        "        is found for the given selector\n",
        "        \"\"\"\n",
        "        selectedElems = bs.select(selector)\n",
        "        if selectedElems is not None and len(selectedElems) > 0:\n",
        "            return '\\n'.join([elem.get_text().replace('\\n','') for elem in selectedElems])\n",
        "        return ''\n",
        "    def getContent(website, path):\n",
        "        \"\"\"\n",
        "        Extract content from a given page URL\n",
        "        \"\"\"\n",
        "        url = website.url+path\n",
        "        bs = Crawler.getPage(url)\n",
        "        if bs is not None:\n",
        "            title = Crawler.safeGet(bs, website.titleTag)\n",
        "            body = Crawler.safeGet(bs, website.bodyTag)\n",
        "            return Content(url, title, body)\n",
        "        return Content(url, '', '')"
      ],
      "metadata": {
        "id": "gDAhYwL9Ct1E"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siteData = [\n",
        "    ['O\\'Reilly', 'https://www.oreilly.com', 'h1', 'div.title-description'],\n",
        "    ['Reuters', 'https://www.reuters.com', 'h1', 'div.ArticleBodyWrapper'],\n",
        "    ['Brookings', 'https://www.brookings.edu', 'h1', 'div.post-body'],\n",
        "    ['CNN', 'https://www.cnn.com', 'h1', 'div.article__content']\n",
        "]\n",
        "websites = []\n",
        "for name, url, title, body in siteData:\n",
        "    websites.append(Website(name, url, title, body))\n",
        "\n",
        "Crawler.getContent(\n",
        "    websites[0],\n",
        "    '/library/view/web-scraping-with/9781491910283'\n",
        "    ).print()\n",
        "Crawler.getContent(\n",
        "    websites[1],\n",
        "    '/article/us-usa-epa-pruitt-idUSKBN19W2D0'\n",
        "    ).print()\n",
        "Crawler.getContent(\n",
        "    websites[2],\n",
        "    '/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/').print()\n",
        "Crawler.getContent(\n",
        "    websites[3],\n",
        "    '/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html'\n",
        "    ).print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dGuNF55DPLf",
        "outputId": "d29c62e2-dd97-4214-88d8-8a35200c1e46"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title:Web Scraping with Python\n",
            "url:https://www.oreilly.com/library/view/web-scraping-with/9781491910283\n",
            "Body:              Book      descriptionLearn web scraping and crawling techniques to access unlimited data from any web source in any format. With this practical guide, you’ll learn how to use Python scripts and web APIs to gather and process data from thousands—or even millions—of web pages at once.Ideal for programmers, security professionals, and web administrators familiar with Python, this book not only teaches basic web scraping mechanics, but also delves into more advanced topics, such as analyzing raw data or using scrapers for frontend website testing. Code samples are available to help you understand the concepts in practice.Show and hide morePublisher resourcesView/Submit Errata\n",
            "title:\n",
            "url:https://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0\n",
            "Body:\n",
            "title:\n",
            "url:https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
            "Body:\n",
            "title:      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu    \n",
            "url:https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
            "Body:New YorkCNN         —                 Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over.                 Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.”     CNN/Adobe StockElon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”                It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics.                 The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.    Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Content:\n",
        "    \"\"\"Common base class for all articles/pages\"\"\"\n",
        "\n",
        "    def __init__(self, topic, url, title, body):\n",
        "        self.topic = topic\n",
        "        self.title = title\n",
        "        self.body = body\n",
        "        self.url = url\n",
        "\n",
        "    def print(self):\n",
        "        \"\"\"\n",
        "        Flexible printing function controls output\n",
        "        \"\"\"\n",
        "        print('New article found for topic: {}'.format(self.topic))\n",
        "        print('URL: {}'.format(self.url))\n",
        "        print('TITLE: {}'.format(self.title))\n",
        "        print('BODY:\\n{}'.format(self.body))"
      ],
      "metadata": {
        "id": "ugeIFiEqDZdd"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    \"\"\"Contains information about website structure\"\"\"\n",
        "\n",
        "    def __init__(self, name, url, searchUrl, resultListing,resultUrl, absoluteUrl, titleTag, bodyTag):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.searchUrl = searchUrl\n",
        "        self.resultListing = resultListing\n",
        "        self.resultUrl = resultUrl\n",
        "        self.absoluteUrl=absoluteUrl\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag"
      ],
      "metadata": {
        "id": "cnYt_AeXJQlS"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crawler:\n",
        "    def __init__(self, website):\n",
        "        self.site = website\n",
        "        self.found = {}\n",
        "\n",
        "    def getPage(url):\n",
        "        try:\n",
        "            html = urlopen(url)\n",
        "        except Exception:\n",
        "            return None\n",
        "        return BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    def safeGet(bs, selector):\n",
        "        \"\"\"\n",
        "        Utility function used to get a content string from a Beautiful Soup\n",
        "        object and a selector. Returns an empty string if no object\n",
        "        is found for the given selector\n",
        "        \"\"\"\n",
        "        selectedElems = bs.select(selector)\n",
        "        if selectedElems is not None and len(selectedElems) > 0:\n",
        "            return '\\n'.join([elem.get_text().replace('\\n','') for elem in selectedElems])\n",
        "\n",
        "    def getContent(self, topic, url):\n",
        "        \"\"\"\n",
        "        Extract content from a given page URL\n",
        "        \"\"\"\n",
        "        bs = Crawler.getPage(url)\n",
        "        if bs is not None:\n",
        "            title = Crawler.safeGet(bs, self.site.titleTag)\n",
        "            body = Crawler.safeGet(bs, self.site.bodyTag)\n",
        "            return Content(topic, url, title, body)\n",
        "        return Content(topic, url, '', '')\n",
        "\n",
        "    def search(self, topic):\n",
        "        \"\"\"\n",
        "        Searches a given website for a given topic and records all pages found\n",
        "        \"\"\"\n",
        "        bs = Crawler.getPage(self.site.searchUrl + topic)\n",
        "        searchResults = bs.select(self.site.resultListing)\n",
        "        for result in searchResults:\n",
        "            url = result.select(self.site.resultUrl)[0].attrs['href']\n",
        "            # Check to see whether it's a relative or an absolute URL\n",
        "            url = url if self.site.absoluteUrl else self.site.url + url\n",
        "            if url not in self.found:\n",
        "                self.found[url] = self.getContent(topic, url)\n",
        "            self.found[url].print()"
      ],
      "metadata": {
        "id": "IvCoyaHcJSgB"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siteData = [\n",
        "    ['Reuters', 'http://reuters.com',\n",
        "     'https://www.reuters.com/search/news?blob=',\n",
        "     'div.search-result-indiv', 'h3.search-result-title a',\n",
        "      False, 'h1', 'div.ArticleBodyWrapper'],\n",
        "    ['Brookings', 'http://www.brookings.edu',\n",
        "     'https://www.brookings.edu/search/?s=',\n",
        "        'div.article-info', 'h4.title a', True, 'h1', 'div.core-block']\n",
        "]\n",
        "sites = []\n",
        "for name, url, search, rListing, rUrl, absUrl, tt, bt in siteData:\n",
        "    sites.append(Website(name, url, search, rListing, rUrl, absUrl, tt, bt))\n",
        "\n",
        "crawlers = [Crawler(site) for site in sites]\n",
        "topics = ['python', 'data%20science']\n",
        "\n",
        "for topic in topics:\n",
        "    for crawler in crawlers:\n",
        "        crawler.search(topic)"
      ],
      "metadata": {
        "id": "45k8rDrUJqFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.targetPattern = targetPattern\n",
        "        self.absoluteUrl = absoluteUrl\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag\n",
        "\n",
        "class Content:\n",
        "    def __init__(self, url, title, body):\n",
        "        self.url = url\n",
        "        self.title = title\n",
        "        self.body = body\n",
        "\n",
        "    def print(self):\n",
        "        print(f'URL: {self.url}')\n",
        "        print(f'TITLE: {self.title}')\n",
        "        print(f'BODY:\\n{self.body}')"
      ],
      "metadata": {
        "id": "HiEw7-ybJvSO"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crawler:\n",
        "    def __init__(self, site):\n",
        "      self.site = site\n",
        "      self.visited = {}\n",
        "\n",
        "    def getPage(url):\n",
        "      try:\n",
        "            html = urlopen(url)\n",
        "      except Exception as e:\n",
        "            print(e)\n",
        "            return None\n",
        "      return BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    def safeGet(bs, selector):\n",
        "      selectedElems = bs.select(selector)\n",
        "      if selectedElems is not None and len(selectedElems) > 0:\n",
        "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
        "      return ''\n",
        "\n",
        "    def getContent(self, url):\n",
        "      \"\"\"\n",
        "      Extract content from a given page URL\n",
        "      \"\"\"\n",
        "      bs = Crawler.getPage(url)\n",
        "      if bs is not None:\n",
        "          title = Crawler.safeGet(bs, self.site.titleTag)\n",
        "          body = Crawler.safeGet(bs, self.site.bodyTag)\n",
        "          return Content(url, title, body)\n",
        "        return Content(url, '', '')\n",
        "\n",
        "    def crawl(self):\n",
        "        \"\"\"\n",
        "        Get pages from website home page\n",
        "        \"\"\"\n",
        "        bs = Crawler.getPage(self.site.url)\n",
        "        targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
        "        for targetPage in targetPages:\n",
        "          url = targetPage.attrs['href']\n",
        "          url = url if self.site.absoluteUrl else f'{self.site.url}{targetPage}'\n",
        "          if url not in self.visited:\n",
        "                self.visited[url] = self.getContent(url)\n",
        "                self.visited[url].print()\n",
        "\n",
        "brookings = Website(\n",
        "    'Brookings', 'https://brookings.edu', '\\/(research|blog)\\/',\n",
        "     True, 'h1', 'div.post-body')\n",
        "crawler = Crawler(brookings)\n",
        "crawler.crawl()"
      ],
      "metadata": {
        "id": "Dt6Bws27M5mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Website:\n",
        "    def __init__(self, name, url, titleTag, bodyTag, pageType):\n",
        "        self.name = name\n",
        "        self.url = url\n",
        "        self.titleTag = titleTag\n",
        "        self.bodyTag = bodyTag\n",
        "        self.pageType = pageType"
      ],
      "metadata": {
        "id": "m_erj-x-M8a1"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Product(Website):\n",
        "    \"\"\"Contains information for scraping a product page\"\"\"\n",
        "    def __init__(self, name, url, titleTag, productNumberTag, priceTag):\n",
        "        Website.__init__(self, name, url, TitleTag)\n",
        "        self.productNumberTag = productNumberTag\n",
        "        self.priceTag = priceTag\n",
        "\n",
        "class Article(Website):\n",
        "    \"\"\"Contains information for scraping an article page\"\"\"\n",
        "    def __init__(self, name, url, titleTag, bodyTag, dateTag):\n",
        "        Website.__init__(self, name, url, titleTag)\n",
        "        self.bodyTag = bodyTag\n",
        "        self.dateTag = dateTag"
      ],
      "metadata": {
        "id": "5VnxjIrOM-H_"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbR-P_IkNBC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}